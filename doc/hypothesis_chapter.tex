\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\setlength{\bibsep}{0.0pt}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{mathtools}
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{wrapfig}


\renewcommand{\refname}{{\large REFERENCES}}

\setlength\parindent{0pt}
\setlength\parskip{5pt}

\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\EX}{\mathbb{E}}

\newenvironment{my_enumerate}{
\begin{enumerate}[G1,leftmargin=0.5cm]
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}}{\end{enumerate}
}

\newenvironment{my_itemize}{
\begin{itemize}[leftmargin=0.5cm]
  \setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}}{\end{itemize}
}


\usepackage{enumitem}
\usepackage[font=small,skip=0pt]{caption}
\usepackage{geometry}
%\geometry{left=0.915cm,top=0.915cm,right=1.27cm,bottom=1.27cm}
\usepackage{nopageno}
\setlength{\intextsep}{-2ex}

\setlength{\belowcaptionskip}{-10pt}
\usepackage{caption}

\title{Hypothesis testing in Metabolic Scaling}
\author{Diego R Barneche, Holly V Moeller, Joey R Bernhardt}

\begin{document}
\maketitle

\section{Introduction}

The Metabolic Theory of Ecology (MTE) can be considered a simple but ``efficient'' theory \citep{marquet2014bioscience}. It is clearly defined in mathematical terms, based on first principles of chemistry and physics, and although it makes simplifying (though explicit) assumptions, it generates a wealth of predictions from relatively few free parameters \citep{marquet2014bioscience, brown2004ecology}. According to MTE, the free parameters that control how body size and temperature determine organismal metabolic rates also yield predictable effects on multiple ecological processes at higher levels of biological organization---e.g., population growth, community biomass, ecosystem elemental residence \citep{brown2004ecology, savage2004amnat, allen2005functecol, barneche2014ele, schramski2015pnas}. For example, the energetic-equivalence hypothesis (or rule) predicts that population density will scale negatively with population mean body size following a power law parametrized by the same size scaling exponent of organismal metabolic rates \citep{damuth1987biollin, allen2002science, barneche2016procb}. These ``predictions'' can be tested in the form of hypotheses---e.g., testing whether a free parameter is of a particular value---and therefore the empirical characterisation of such parameters constitutes a critical component in the process of hypothesis testing, regardless of the level of organization at which a prediction is being made (e.g., individual, population, community, ecosystem).

For example, scientists have debated for over a century whether the free parameter that governs the size scaling of metabolic rates (parameter $\alpha$ in equation \ref{eq1} below) is 2/3 or 3/4. Early work by Max Rubner in the 1880s found that mammalian basal metabolic rate scaled with body size with a scaling exponent of $\alpha$ = 2/3, a value which was attributed to the dissipation of heat from the body surface and the geometric scaling of body surface to volume (or mass) \citep{rubner1883zeibiol}. Subsequent studies, including those by Max Kleiber \citeyearpar{kleiber1932hilgardia}, observed and/or mathematically predicted higher values of the scaling exponents, closer to 3/4---a.k.a., ``Kleiber's Law'' \citep{west1997science, west1999science}. To further muddy the waters, more recent studies have reported multiple values for the size scaling exponent depending on different taxonomic, phylogenetic and functional groups, suggesting that there might not be a single canonical or universal scaling exponent \citep[e.g.,][]{white2003pnas, white2005jeb, delong2010pnas}\citep[but see][for recent extensions of theoretical predictions]{brummer2017plosbiol}. So, is there a single, correct value, and how would we know?

The debate surrounding parameter values that are predicted from theory is in part dependent on how hypothesis testing is conducted. This entails not only the statistical philosophy (e.g., frequentist or Bayesian), but also the statistical model structure. Over the years, many different statistical techniques have been used to distinguish among competing parameter value expectations while accounting for a multitude of covariates and confounding factors. Traditionally, most of these have been based on methods belonging to frequentist statistics, based on \emph{p}-value or 95\% confidence intervals. However, the empirical measurements (e.g., metabolic rates of individuals of different sizes) that are used to statistically characterize free parameters---and therefore test hypotheses---may inherit noise that might be attributable to, for example, the measurement instrument, the experimental design, the observer, or intrinsic individual attributes. Therefore, more often than not, statistical parameter estimates will carry uncertainty which affects the capacity of the observer to distinguish among competing parameter value expectations. In the MTE context, parameter uncertainty takes additional relevance because predictions at higher levels of biological organization suddenly can no longer rely on a parameter point estimate, but rather on a full distribution of possibilities.

Bayesian statistics provides a powerful way to probabilistically assess parameter values \citep{kruscke2018psybullrev,gelman2020book}. In doing so, it can be used to properly propagate parameter uncertainty between processes at different levels of biological organization \citep[not discussed here, but see e.g.,][for examples that are relevant to MTE]{barneche2014ele, barneche2016procb} because parameter estimates encompass entire posterior distributions and not just single point estimates. Posterior distributions represent direct probability statements about a parameter, and, in that sense, hypotheses are not longer punctual---e.g., is a free parameter greater than zero?---and should be re-phrased in probabilistic terms---e.g., what is the probability of the free parameter being greater than zero? Moreover, Bayesian statistics provides a convenient way to compare competing prior expectations of parameter values in the language of evidence \citep{benjamin2017nathumbeh,
lakens2022tree}. In this chapter, we begin by illustrating how MTE's master equation can be manipulated to generate predictions---and therefore hypotheses---at the organismal level. We then focus on some of the approaches that have been used to conduct hypothesis testing arising from MTE models, with a focus on Bayesian statistics. Finally, we also provide general pointers for those who are planning to conduct experiments targeted at testing particular values of metabolic scaling parameters.

\bigskip
\bigskip

\section{Manipulating MTE's master equation for hypothesis testing}
\label{exploring}

Arguably, one of the greatest intellectual contributions of the Metabolic Theory of Ecology (MTE) was the direct manipulation of its master equation to generate empirical hypothesis tests. MTE's master equation can be defined as:

\begin{equation}
  B = b_0(T_c) M^\alpha f(T),
  \label{eq1}
\end{equation}

where $B$ is organismal metabolic rate, $b_0(T_c)$ is the normalization constant at a standardised absolute temperature, $T_c$ (K), $M$ is body mass and $\alpha$ is the dimensionless size scaling exponent. The function $f(T) = A(T)/S(T)$ sets the temperature dependence of $B$, whereby 

\begin{equation}
    A(T) = e^{\frac{E_a}{k}\left(\frac{1}{T_c} - \frac{1}{T}\right)}
    \label{eqAB}
\end{equation} 

is the Boltzmann-Arrhenius relationship \citep{gillooly2001science}, $T$ is absolute temperature (K), $E_a$ (eV) is an activation energy, $k$ is the Boltzmann constant ($8.62 \times 10^{-5}$ K / eV), and

\begin{equation}
    S(T) = 1 + \left(\frac{E_a}{E_i - E_a}\right) e^{\frac{E_i}{k}\left(\frac{1}{T_{opt}} - \frac{1}{T}\right)}
    \label{eqSS}
\end{equation} 

is a modified Schoolfield-Sharpe equation \citep{schoolfield1981, barneche2014ele}, $E_i$ (eV) is a phenomenologically-defined inactivation energy, and $T_{opt}$ (K) is the optimum temperature, i.e., the temperature at which $B$ is maximum (Figure \ref{fig1}).

Earlier MTE studies often considered a simplified $f(T) = A(T)$ by assuming that organisms should primarily inhabit environments below $T_{opt}$. Based on such formulation, empirical evidence across multiple species suggested an average activation energy $E_a$ = 0.65 eV \citep{gillooly2001science, brown2004ecology}, allowing one to calculate a ``temperature-corrected'' metabolic rate, $Y = B / A(T)$ (Figure \ref{fig1}, left). In this formulation, body mass is the sole predictor, and a linear equation can be obtained by transforming the equation to the natural logarithmic scale:

\begin{equation}
  \textrm{ln}Y = \textrm{ln}b_0(T_c) + \alpha \textrm{ln}M.
  \label{eq3}
\end{equation}

From the point of view of a statistical linear regression, $\textrm{ln}Y \sim \textrm{ln}M$, $\textrm{ln}b_0(T_c)$ and $\alpha$ would be equivalent to the intercept and slope, respectively. $B$ can also be corrected by body mass instead, such that the sole predictor of the response would be temperature \citep[Figure \ref{fig1}, right;][]{brown2004ecology, savage2004amnat}:

\begin{equation}
  \textrm{ln}Z = \textrm{ln}b_0(T_c) + \frac{E_a}{k}\left(\frac{1}{T_c} - \frac{1}{T}\right),
  \label{eq4}
\end{equation}

where $Z = B / M^\alpha$. By translating the original mathematical equations into statistical models, one can explicitly test for parameter values such as $\alpha$ = 0.75 (equation \ref{eq3}) or 0.6 eV $< E_a <$ 0.7 eV (equation \ref{eq4}) \citep{west1997science, gillooly2001science, brown2004ecology}. That said, over the years the usage of canonical values for both $\alpha$ and $E_a$ have been called into question because these have been shown to vary substantially among organisms and other factors \citep{makarieva2005functecol, glazier2005biolrev, moses2008amnat, glazier2010biolrev, pawar2016amnat, barton2019limn, white2003pnas, white2005jeb, delong2010pnas}. Surprisingly, however, the normalisation constant $b_0(T_c)$ has been treated as highly variable since earlier work \citep{brown2004ecology}. Therefore, one can take an agnostic approach by i) not assuming any particular value of either scaling parameter (i.e., $\alpha$ and $E_a$), and ii) jointly estimating both in a multiple regression framework \citep[i.e., using equations \ref{eq1}--\ref{eqSS};][]{barneche2014ele}.

\bigskip
\bigskip

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=1\textwidth]{../output/figure_1.pdf}
  \end{center}
  \caption{Theoretical curves illustrating the master equation of metabolic rates as defined in equation \ref{eq1}. Left: Temperature-corrected metabolic rates (see equation \ref{eq3}). Right: Mass-corrected metabolic rates (see equation \ref{eq4}). Figure code available on GitHub: \url{https://github.com/dbarneche/hts}.}
  \label{fig1}
\end{figure}

\bigskip
\bigskip

\section{Bayesian framework in metabolic scaling hypothesis testing}
\label{bayesian}

How do we actually test a hypothesis in metabolic scaling? How can we ascertain whether a parameter is, for example, 3/4 and not 2/3 or something else entirely? Different statistical philosophies will provide different approaches, and here we avoid discussing hypothesis testing from a frequentist inference approach because there have been many debates and critiques over the decades about the misuse and misinterpretation of \emph{p}-values \citep[for recent discussions please see][]{wasserstein2016amstat, benjamin2017nathumbeh, amrhein2019nat, muff2022tree, lakens2022tree, muff2022responsetree}. Instead, in Bayesian inference \citep{kruschke2010tree}, given a prior belief, the posterior distribution provides a direct credibility assessment of a model or its parameters, which is often the implicit goal of statistical inference.

Credibility assessments are quite flexible and can be done for one or more parameters simultaneously, and over different ranges of values. For example, by fitting data to equation \ref{eq3} in a Bayesian framework, one can use the posterior distribution of $\alpha$ to assess whether $\alpha = 0.75 \pm 0.25$, or the probability of $0.5 < \alpha < 1$. Such credibility assessments are done in terms of exceedance probabilities, i.e., the proportion of the posterior distribution the falls within an expected parameter range. Similarly, credibility assessments can be made for joint parameter estimates because the covariance between parameters is an emergent property of most Bayesian modelling algorithms \citep{kruschke2010tree}. So, for example, in an long-term metabolic evolution experiment, one could test whether there are trade-offs (observable covariance) between $b_0$ and $E_a$ or $T_{opt}$ \citep[i.e., the temperature at which metabolic rate is at its maximum;][]{schoolfield1981, pawar2016amnat, barton2020ele}. Regardless of the number of parameters or their expected range (hypothesis), credibility assessments can also be done via hard-and-fast, Null Hypothesis Significance Testing (NHST)-inspired rules based on credibility intervals (e.g., does the 95\% highest density interval of $\alpha$'s posterior distribution include 0.75? \citep{barneche2014ele, barneche2018ele} or even Regions of Practical Equivalence (ROPE) based on theory \citep{kruscke2018psybullrev, nunes2021functecol} to avoid comparing a posterior distribution against a single point estimate \cite[however, please note that the ROPE concept has been recently called into question;][]{schwaferts2020report}.

In addition to credibility assessments based on a single model's parameter posterior distribution, Bayesian inference allows for the calculation of evidence ratios between two competing models or parameter values via Bayes factors. This is in fact an extremely useful, albeit to the best of our knowledge, underutilized technique which might have directly contributed early on to debates surrounding the universality of metabolic scaling parameters \citep[e.g., is $\alpha$ = 3/4 or 2/3;][]{west1999science, glazier2005biolrev, moses2008amnat}. Bayesian inference allows researchers to evaluate alternative hypotheses for the value of the scaling parameter. Consider for instance two empirical fittings of equation \ref{eq1}, a null fitting ($M_1$) in which the prior belief of the mass scaling parameter is $\alpha \sim \mathcal{N}(0.67, 0.01)$, and an alternative ($M_2$) in which $\alpha \sim \mathcal{N}(0.75, 0.01)$ (the priors for all other parameters are held constant across both models). The Bayes factor of $M_1$ versus $M_2$, $F$, is defined as the ratio between the posterior odds, $p(M_1 | D)/p(M_2 | D)$, and the prior odds, $p(M_1)/p(M_2)$:

\begin{equation}
\begin{split}
  F = \frac{p(M_1 | D)}{p(M_2 | D)} / \frac{p(M_1)}{p(M_2)} \\
  \frac{p(M_1 | D)}{p(M_2 | D)} = F \frac{p(M_1)}{p(M_2)},
  \label{eq5}
\end{split}
\end{equation}

where $D$ is the observed data, $p(M_\ast | D)$ is the posterior probability of model $M_\ast$ given $D$, and $p(M_\ast)$ is the prior probability of $M_\ast$. Equation \ref{eq5} allows one to calculate the strength of evidence between $M_1$ and $M_2$ \citep{benjamin2017nathumbeh, kruscke2018psybullrev, lakens2022tree}. Here we apply this concept to the data from \cite{barneche2019functecol}---the code for analysis and plots is available on GitHub (\url{https://github.com/dbarneche/hts})---which originally yielded an estimate of $\alpha = 0.65$ for the size scaling of metabolic rates in zebrafish. For the sake of this example, we assume that there is no knowledge about the prior probability of each model, i.e., they both have a 0.5 probability of being the best model and therefore $p(M_1)/p(M_2) = 1$ (Figure \ref{fig2})---however, please note that different prior probabilities could have been assigned via, for example, a meta-analysis to determine which parameter value has had the strongest precedence. According to equation \ref{eq5}, in such case where $p(M_1)/p(M_2) = 1$, the posterior odds are equivalent to the Bayes factor, and in this example data, $F \approx 85$ in favour of model $M_1$.

\bigskip
\bigskip

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=1\textwidth]{../output/figure_2.pdf}
  \end{center}
  \caption{Model selection using Bayes factors as a way to disentangle theoretically predicted mass scaling exponent values. Competing models (red, $M_1$; blue, $M_2$) differ only in their prior for the mass scaling exponent, as explained in the text. Top-left plot shows the equal prior model probabilities, and top-right plot shows the posterior model probabilities after fitting data from \cite{barneche2019functecol}, which originally yielded an $\alpha = 0.65$ estimate. Together these top plots provide strong evidence in favor of model $M_1$. Bottom plots show both the prior (left) and posterior (right) distributions of $\alpha$. Figure and analysis code available on GitHub: \url{https://github.com/dbarneche/hts}.}
  \label{fig2}
\end{figure}

\bigskip
\bigskip

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=1\textwidth]{../output/figure_3.pdf}
  \end{center}
  \caption{Hypothetical data illustrating a canonical case of metabolic rates as a function of individual body mass. Left: data plotted on natural scale, overlaid with posterior predictions obtained from a Bayesian non-linear model using a Gamma distribution. Right: same data plotted on the natural logarithmic scale, overlaid with posterior predictions obtained from a Bayesian linear model using a Gaussian distribution. Top: original parameters used to simulate the data following equation \ref{eq3}. Inset estimates represent the mean fits for each parameter as well as their Bayesian 95\% credible intervals (95\% C.I.). Grey polygons represent Bayesian predictions across 4,000 posterior draws, and dashed red lines represent the mean predictions. Figure and analysis code available on GitHub: \url{https://github.com/dbarneche/hts}.}
  \label{fig3}
\end{figure}

\bigskip
\bigskip

\section{To linearize or not to linearize? Choosing transformations and distributions}
\label{distributions}

Many physiological rates such as growth or reproductive rates can encompass multiple orders of magnitude, and on the logarithmic scale they can be continuous and boundless. Therefore, their empirical distributions can be well characterized by either a Gaussian or a Student-t distribution. These statistical distributions will by default assume constant variance around the mean, and therefore the error distribution on the original data scale should ideally be heteroscedastic multiplicative \citep[see, e.g.,][Figure \ref{fig3}]{kerkhoff2009jtheorbiol, xiao2011ecology}. An alternative but less frequently adopted possibility would be to model the data directly on its natural scale by fitting a non-linear model. The nature of the data and its error are again key. For instance, a response variable that is positive continuous (i.e., does not include 0) and whose error is heteroscedastic multiplicative could in principle be modelled using a Gamma distribution with an identity link. Nowadays, multiple statistical packages make it relatively easy for novice users to fit both linear and non-linear models in \texttt{R} \citep{Rcitation} (e.g., \texttt{lme4} \cite{lme4}, \texttt{nlme} \cite{nlme}, \texttt{glmmTMB} \cite{glmmTMB}, \texttt{brms} \cite{brms}), and combined these offer great flexibility in terms of choosing an appropriate error distribution. For an overview on statistical distributions belonging to the exponential family, please see \cite{zuur2009book}.

\bigskip
\bigskip

\section{Multilevel models to reconcile generality and idiosyncrasies in scaling}

A major contention throughout the history of MTE has been the applicability of parameter estimates obtained from macro-ecological meta-analyses versus estimates obtained at the intra-specific level \citep{brody1945book, peters1983book, agutter2004theor, glazier2005biolrev, moses2008amnat, hirst2014ele, barneche2015pnas}. Most of these critiques arose from the fact that two foundational models in MTE, namely the West-Brown-Enquist fractal scaling model \citep{west1997science} and the Ontogenetic Growth model \citep{west2001nature}, were derived from the perspective of individual organisms despite the fact that most of their empirical tests relied on data pooled across species of different body sizes \citep[e.g.,][]{brown2004ecology, savage2004amnat}.

Theoretical debates aside, this problem illustrates a classical issue of within- versus between-group effects in statistics that can be solved with multilevel modelling. In the context of metabolic scaling, multilevel models (a.k.a. hierarchical, random effects or mixed effect models) can aid in assessing the consistency of scaling phenomena within and across taxa \citep[e.g.,][]{glazier2005biolrev, isaac2010ele, hirst2014ele, barneche2015pnas} by partitioning the variance into general trends, represented by `fixed' or `population-level` variables, and individual- or taxon-specific idiosyncrasies, represented by `random' or `group-level' variables \citep{vandepol2009animbehav, allgeier2015pnas, padfield2016ecollett, barneche2019functecol, garzke2019plosbiol, barton2020ele}. A good visual example of such partitioning can be found in Figure 1 from \cite{vandepol2009animbehav}. Assuming that appropriate data exists such that a nested structure can be obtained in the data (e.g., multiple measurements of individuals throughout ontogeny, multiple individuals within species, and/or multiple individuals across species), overall metabolic parameters are then allowed to vary following `hyper-parameters' (e.g., the inter-individual standard deviation of $\alpha$), and one can establish the magnitude of discrepancy between the overall or fixed parameter location (e.g., mean) and the group-level representatives (e.g., individuals or species).

While the variance captured by group-level variables does not by itself identify particular driving mechanisms of difference, it does highlight areas for future investigation. Alternatively, this group-level structure can be further extended into a proper ``animal'' pedigree to evaluate genetic variation in scaling parameters among individuals of the same species \citep{careau2015procb}, or even a full phylogenetic tree which evaluates the rate of parameter evolution across species---please note, however, that parameter diversification dynamics remains an actively debated and unresolved issue \citep{louca2020nature}. That said, the major limitation to achieving parameter estimates with high resolution at multiple levels of biological organization is data quality. However, the recent rise of high-throughput methods to evaluate physiological rates has made large-scale ecophysiology and evolution studies possible \citep{padfield2016ecollett, barneche2019functecol, malerba2018newphy, garzke2019plosbiol, barton2020ele}, and we now turn our attention to experimental designs in the metabolic scaling space.

\section{General advice for experimental design}

Being an efficient theory \citep{marquet2014bioscience}, MTE yields many predictions that are directly and empirically testable with controlled experiments. For example, given a fixed amount of resources, MTE predicts that a population carrying capacity should decrease with temperature because of increased per-capita metabolic demands \citep{brown2004ecology, savage2004amnat, barneche2016procb} at warmer temperatures. Furthermore, an actual value for the rate of decline (i.e., the hypothesis) can be tested based on the (known or assumed) temperature dependence of metabolic rates at the individual level \citep[see e.g.,][]{savage2004amnat}, and this hypothesis can be tested with experiments that measure maximal population size as a function of temperature \citep[see e.g.,][and many more examples in the chapter \textbf{``Experimental Approaches in Metabolic Scaling Theory''}]{bernhardt2018amnat, bernhardt2018procb}. But how should one plan for experiments in general?

Experimental design planning is a key component of a successful hypothesis testing exercise. When designing an experiment, extra care must be taken to control for unwanted and/or confounding variables, and to make sure that the sample size is enough to test a particular hypothesis---i.e., the statistical ``power''---given a parameter value and its expected variance \citep{gelman2020book}. Often, the major hurdle in determining an ideal sample size is to define both the ``effect size'' (i.e., the parameter value) and how much it varies. Luckily, theories such as MTE conveniently alleviate these barriers because 1) they already predict mean parameter values (i.e., the hypothesis), and 2) the extensive literature and empirical tests of MTE allow for a reliable calculation of expected parameter variance. Consequently, one may easily pre-register their specific hypothesis---including parameter values---and experimental design, furthering the transparency and reproducibility of science \citep{odea2021bmcbiol}.

Finally, we recommend caution when using statistical interactions, e.g., does the effect of predictor variable A on the response variable change in sign or magnitude according to variable B? Statistical interactions have been heavily abused and misinterpreted \citep[see Section 16.4 of][]{gelman2020book}, including in the ecological literature \citep{spake2023biolrev}, and some of us have been guilty of this issue in the past. What many researchers do not realize, however, is that statistical tests for interactions are data hungry, and are often evaluated by under-powered studies. This is a problem that can lead to highly skewed effect sizes and uncertainty estimates, including of the wrong magnitude and sign \citep{gelman2014perspsysci}. In metabolic scaling, for example, there is a reignited debate about whether $\alpha$ itself is temperature dependent \citep{glazier2020jcompphysiolb}. Although this is an extremely interesting academic question with potential implications to how we estimate carbon fluxes under climate change, caution must be exercised to make sure that the estimates of statistical interactions (e.g., does $\alpha$ change with temperature) are not mere artifacts of under-powered datasets / analyses.

\section{Concluding remarks}
Empirical tests of the Metabolic Theory of Ecology (MTE) allow us to strengthen our understanding of the theory and its tenets. Here we offered a fresh perspective on how to estimate free parameters in MTE (or any other theory in general) using Bayesian statistics, as well as how to distinguish among competing hypotheses using evidence. Such approaches could have helped resolve past (and we hope that they may also help future) debates regarding parameter values and their associated uncertainty because the stakes might be high. MTE is a powerful tool to understand critical elemental cycles, and therefore we should do our best to estimate parameters in the best way possible, particularly in the climate-changing Anthropocene. Bayesian statistics offer a convenient way to not only state a parameter mean estimate, but also its uncertainty, or in other words, how much we do not know. As additional statistical frameworks, including Bayesian approaches, become commonplace, ecologists have a growing number of tools to approach experimental design and hypothesis testing. Customized simulations can be used to identify the sample size necessary to achieve robust statistical power. Special care should be taken when designing multi-level experiments to ensure that there is sufficient power for detecting main effects as well as interactions. As is becoming widely accepted best-practice in the field, we urge researchers to consider their analysis plan prior to beginning experimental work, and ideally they should pre-register their hypotheses and experimental design.

\bibliographystyle{apalike}
\bibliography{refs}

\end{document}
